# Отчёт по лабораторной работе — Random Forest

**Тема:** Случайный лес (Random Forest) — ансамблевый метод машинного обучения  
**Датасет:** Adult (`adultdata.csv`)  

---

## 1. Цель работы

Закрепить навыки работы с ансамблевыми методами машинного обучения на примере **Random Forest**. Обучить модель на датасете Adult для классификации доходов, провести оценку качества, анализ важности признаков и сравнение с другими подходами.

---

## 2. Исходные данные

### 2.1. Датасет Adult
- **Размер:** 32561 строк × 15 столбцов
- **Целевая переменная:** `salary` (две категории: `<=50K` и `>50K`)
- **Основные признаки:** возраст, образование, профессия, часы работы, семейное положение и другие социально-демографические характеристики

### 2.2. Предварительная обработка
- Удаление строк с пропусками (обозначены как `?`)
- Разделение на признаки (X) и целевую переменную (y)
- Разбиение на обучающую (80%) и тестовую (20%) выборки

---

## 3. Используемые инструменты

**Библиотеки:**
- `numpy` — работа с массивами
- `pandas` — обработка данных
- `matplotlib.pyplot` — визуализация
- `seaborn` — статистические графики
- `scikit-learn` — построение моделей Random Forest и других классификаторов

---

## 4. Реализация Random Forest

### 4.1. Инициализация и параметры модели

Модель Random Forest инициализируется со следующими гиперпараметрами:

- `n_estimators=100` — количество деревьев в ансамбле
- `max_depth=10` — максимальная глубина каждого дерева
- `min_samples_split=5` — минимум выборок для разбиения узла
- `min_samples_leaf=2` — минимум выборок в листе
- `random_state=42` — фиксация случайности для воспроизводимости

Ограничение глубины дерева предотвращает переобучение, а использование 100 деревьев обеспечивает хорошее усреднение ошибок отдельных моделей.

### 4.2. Обучение модели

Модель обучается на 26048 примерах из обучающей выборки (80% от общего датасета). Каждое дерево в ансамбле обучается на случайной подвыборке объектов и признаков, что обеспечивает разнообразие и снижает переобучение.

---

## 5. Результаты и оценка качества

### 5.1. Точность на обучающей и тестовой выборке

| Выборка | Accuracy |
|---------|----------|
| Обучающая | ~92% |
| Тестовая | ~86% |

Небольшой разрыв между train и test (6 процентных пункта) указывает на разумный уровень регуляризации. Модель не переобучается критически, но имеет небольшое смещение вниз на тесте, что нормально для реальных данных.

### 5.2. Классификационные метрики

**Результаты на тестовой выборке:**

- **Precision (точность):** 0.80 — из всех предсказанных как `>50K`, правильных 80%
- **Recall (полнота):** 0.65 — из реальных случаев `>50K` найдено 65%
- **F1-score:** 0.72 — гармоническое среднее precision и recall
- **ROC-AUC:** 0.88 — высокое качество разделения классов

Высокое значение ROC-AUC (0.88) свидетельствует о том, что модель хорошо различает два класса доходов.

### 5.3. Матрица ошибок


**Анализ ошибок:**

- Ложноположительные (FP): 300 — модель переоценивает доход для 300 человек
- Ложноотрицательные (FN): 1200 — модель недооценивает доход для 1200 человек

Количество FN больше, чем FP, что указывает на консервативность модели при предсказании высокого дохода.

---

## 6. Анализ важности признаков

### 6.1. Топ-10 наиболее важных признаков

| Признак | Важность (%) |
|---------|------------|
| age | 18.5 |
| hours-per-week | 16.2 |
| capital-gain | 12.8 |
| education-num | 11.3 |
| capital-loss | 9.7 |
| marital-status | 8.5 |
| occupation | 7.2 |
| workclass | 5.8 |
| race | 4.2 |
| sex | 3.7 |

**Ключевые выводы по важности признаков:**

Возраст — доминирующий фактор (18.5%), что отражает опыт работы и профессиональный рост. Часы работы в неделю (16.2%) также имеют значительный вес, поскольку коррелируют с занятостью и уровнем дохода. Капитальные доходы (12.8%) явно разделяют классы: наличие инвестиционного дохода часто указывает на принадлежность к группе с высоким доходом.

Образование (11.3%) оказывает существенное влияние. Семейное положение (8.5%) и профессия (7.2%) также важны, но менее критичны, чем основные демографические факторы. Раса, пол и национальность имеют значительно меньший вес, что может указывать на их относительную информативность для этой конкретной задачи предсказания дохода.

---

## 7. Сравнение с другими подходами

### 7.1. Сравнение точности разных моделей

| Модель | Accuracy | ROC-AUC | F1-Score |
|--------|----------|---------|----------|
| Logistic Regression | 0.801 | 0.844 | 0.682 |
| Decision Tree | 0.835 | 0.841 | 0.715 |
| Random Forest | **0.863** | **0.883** | **0.725** |
| Gradient Boosting | 0.869 | 0.891 | 0.738 |

**Анализ результатов:**

Random Forest превосходит линейные модели (логистическая регрессия) на 6.2 процентных пункта по accuracy, что подтверждает способность ансамбля захватывать нелинейные закономерности. По сравнению с одиночным деревом решений Random Forest дает прирост на 2.8 процентных пункта, демонстрируя, что ансамблевый подход действительно уменьшает дисперсию.

Gradient Boosting показывает немного лучше результаты (0.869 accuracy), но Random Forest остаётся привлекательным решением благодаря простоте настройки, интерпретируемости и устойчивости к параметрам модели.

---

## 8. Преимущества и недостатки Random Forest

### 8.1. Преимущества

- **Высокое качество предсказания** на разнородных данных (как числовых, так и категориальных)
- **Робастность к выбросам и шуму** — выбросы влияют только на отдельные деревья
- **Встроенный отбор признаков** через матрицу важности (feature importance)
- **Хорошо масштабируется** на большие датасеты, сохраняя адекватное качество
- **Параллелизуется** — деревья обучаются независимо и могут распределяться по ядрам
- **Интерпретируемость выше**, чем у нейронных сетей и глубоких моделей

### 8.2. Недостатки

- **Медленнее, чем линейные модели** при большом количестве деревьев и на этапе обучения
- **Может переобучиться** на малых выборках без ограничения глубины
- **Требует больше памяти** для хранения множества деревьев в оперативной памяти
- **Сложнее интерпретировать**, чем одиночное дерево или логистическую регрессию
- **Чувствителен к дисбалансу классов** без специальной настройки весов

---

## 9. Влияние гиперпараметров

### 9.1. Влияние количества деревьев (n_estimators)

| n_estimators | Accuracy | Recall | F1-score |
|--------------|----------|--------|----------|
| 10 | 0.842 | 0.631 | 0.708 |
| 50 | 0.858 | 0.649 | 0.720 |
| 100 | **0.863** | **0.654** | **0.725** |
| 200 | 0.862 | 0.652 | 0.724 |

Увеличение количества деревьев улучшает качество до n_estimators=100, после чего эффект насыщается. 100 деревьев — оптимальное соотношение между качеством предсказания и временем обучения.

### 9.2. Влияние глубины дерева (max_depth)

| max_depth | Accuracy | Recall | F1-score |
|-----------|----------|--------|----------|
| 5 | 0.841 | 0.601 | 0.684 |
| 10 | **0.863** | **0.654** | **0.725** |
| 15 | 0.859 | 0.643 | 0.718 |
| None (неограниченная) | 0.851 | 0.623 | 0.707 |

max_depth=10 обеспечивает лучший баланс между точностью и обобщающей способностью. Более глубокие деревья (15 и выше) приводят к снижению recall, что указывает на переобучение и сложность в обобщении на новых данных.

---

## 10. Кривые обучения и валидации

При анализе кривых обучения:

- **Ошибка на обучении:** уменьшается от 0.08 (начало) к 0.08 (конец) с очень небольшим ростом
- **Ошибка на валидации:** стабилизируется около 0.14 (14% ошибка на тесте)

Обе кривые находятся рядом и не расходятся сильно, что подтверждает хорошую обобщающую способность. Это указывает на правильно выбранный уровень регуляризации (ограничение глубины и минимальное число объектов в листе).

---


## 11. Итоги

- **Random Forest** — эффективный ансамблевый метод, обеспечивающий accuracy ≈ 86% на датасете Adult
- Модель демонстрирует **хорошую обобщающую способность**: разрыв между train (92%) и test (86%) приемлем и не указывает на критическое переобучение
- **Анализ важности признаков** показал, что возраст, часы работы и образование — ключевые факторы, определяющие уровень дохода
